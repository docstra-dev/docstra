# File: ./docstra/core/documentation/generator.py

import os
import re
import json
import subprocess
from pathlib import Path
from typing import (
    Dict,
    List,
    Any,
    Optional,
    Set,
    Union,
    Tuple,
    Collection,
    cast,
)
import yaml
import datetime
import concurrent.futures
import uuid

from docstra.core.document_processing.document import (
    Document,
    DocumentMetadata,
    DocumentType,
)
from docstra.core.indexing.repo_map import RepositoryMap
from docstra.core.retrieval.chroma import ChromaRetriever


class DocumentationGenerator:
    """Generate documentation for code files using MkDocs."""

    def __init__(
        self,
        llm_client: Any,
        output_dir: Union[str, Path],
        format: str = "mkdocs",
        repo_map: Optional[RepositoryMap] = None,
        exclude_patterns: Optional[List[str]] = None,
        chroma_retriever: Optional[ChromaRetriever] = None,
        documentation_structure: str = "file_based",
        module_doc_depth: str = "full",
        llm_style_prompt: Optional[str] = None,
        theme: str = "material",
        project_name: str = "Project Documentation",
        project_description: str = "Documentation generated by Docstra",
        project_version: str = "0.1.0",
        incremental: bool = False,
        console: Optional[Any] = None,
    ):
        """Initialize the documentation generator.

        Args:
            llm_client: LLM client for generating documentation
            output_dir: Output directory for documentation
            format: Output format (mkdocs, markdown, html)
            repo_map: Optional repository map for enhanced navigation
            exclude_patterns: Optional patterns to exclude from documentation
            chroma_retriever: Optional ChromaDB retriever for contextual information
            documentation_structure: Strategy for organizing docs ("file_based" or "module_based")
            module_doc_depth: Depth of documentation for modules ("overview_only" or "full")
            llm_style_prompt: Optional user-defined prompt to guide LLM documentation style
            theme: Documentation theme
            project_name: Project name for documentation
            project_description: Project description
            project_version: Project version
            incremental: Whether to perform incremental update
            console: Optional console for output
        """
        self.llm_client = llm_client
        self.output_dir = Path(output_dir)
        self.format = format.lower()
        self.repo_map = repo_map
        self.exclude_patterns = exclude_patterns or []
        self.chroma_retriever = chroma_retriever
        self.documentation_structure = documentation_structure
        self.module_doc_depth = module_doc_depth
        self.llm_style_prompt = llm_style_prompt

        # Store the new parameters
        self.theme = theme
        self.project_name = project_name
        self.project_description = project_description
        self.project_version = project_version
        self.incremental = incremental
        self.console = console

        # Track processed documents and metadata with proper type annotations
        self.nav_items: List[Dict[str, str]] = []
        self.processed_files: Set[str] = set()
        self.documents_by_path: Dict[str, Document] = {}
        self.modules: Dict[str, List[Dict[str, Any]]] = {}
        self.global_symbols: Dict[str, List[str]] = {}

        # Set up output directories
        self.docs_dir = self.output_dir
        self.assets_dir = self.docs_dir / "assets"
        self.css_dir = self.assets_dir / "css"
        self.js_dir = self.assets_dir / "js"
        self.config_dir = self.docs_dir / ".docstra"

        # Create necessary directories
        os.makedirs(self.docs_dir, exist_ok=True)
        os.makedirs(self.assets_dir, exist_ok=True)
        os.makedirs(self.css_dir, exist_ok=True)
        os.makedirs(self.js_dir, exist_ok=True)
        os.makedirs(self.config_dir, exist_ok=True)

        # Load existing configuration if available
        self._load_configuration()

    def _load_configuration(self) -> None:
        """Load existing configuration and state."""
        config_file = self.config_dir / "config.json"
        if config_file.exists():
            try:
                with open(config_file, "r") as f:
                    config = json.load(f)
                    self.processed_files = set(config.get("processed_files", []))
                    self.nav_items = config.get("nav_items", [])
                    self.modules = config.get("modules", {})
                    self.global_symbols = config.get("global_symbols", {})
            except Exception as e:
                print(f"Warning: Could not load configuration: {e}")

    def _save_configuration(self) -> None:
        """Save current configuration and state."""
        config = {
            "processed_files": list(self.processed_files),
            "nav_items": self.nav_items,
            "modules": self.modules,
            "global_symbols": self.global_symbols,
            "last_updated": datetime.datetime.now().isoformat(),
        }

        config_file = self.config_dir / "config.json"
        try:
            with open(config_file, "w") as f:
                json.dump(config, f, indent=2)
        except Exception as e:
            print(f"Warning: Could not save configuration: {e}")

    def _build_comprehensive_context(self, document: Document) -> Dict[str, Any]:
        """Build comprehensive context for documentation generation.

        Args:
            document: Document to generate context for

        Returns:
            Dictionary containing comprehensive context
        """
        context = {
            "file_info": {
                "path": document.metadata.filepath,
                "language": document.metadata.language,
                "size": document.metadata.size_bytes,
                "symbols": document.metadata.classes + document.metadata.functions,
                "imports": document.metadata.imports,
            },
            "module_info": {},
            "dependencies": [],
            "related_files": [],
            "code_quality": {},
            "documentation_stats": {},
            "retrieved_contextual_chunks": [],
        }

        if self.repo_map:
            file_node = self.repo_map.find_file(document.metadata.filepath)
            if file_node:
                # Get module category and related files
                module_category = self.repo_map._categorize_module(
                    document.metadata.filepath
                )
                related_files = self.repo_map.get_related_files(
                    document.metadata.filepath
                )
                dependencies = self.repo_map.get_file_dependencies(
                    document.metadata.filepath
                )

                # Update module information
                context["module_info"] = {
                    "category": module_category,
                    "complexity": file_node.complexity,
                    "line_count": file_node.line_count,
                    "contributors": file_node.contributors,
                    "last_modified": file_node.last_modified,
                    "tags": file_node.tags,
                }

                # Update dependencies and related files
                context["dependencies"] = dependencies
                context["related_files"] = related_files

                # Update code quality metrics
                context["code_quality"] = file_node.analysis["code_quality"]
                context["documentation_stats"] = {
                    "coverage": file_node.analysis["documentation_coverage"],
                    "test_coverage": file_node.analysis["test_coverage"],
                }

                # Get module overview if available
                module_overview = self.repo_map.get_module_overview()
                if module_overview:
                    context["module_overview"] = {
                        "statistics": module_overview["statistics"],
                        "modules": module_overview["modules"].get(module_category, []),
                        "dependencies": module_overview["dependencies"].get(
                            document.metadata.filepath, []
                        ),
                        "complexity": module_overview["complexity"].get(
                            document.metadata.filepath, None
                        ),
                    }

        # Retrieve contextual chunks using ChromaRetriever
        if self.chroma_retriever and document.content:
            try:
                # 1. Retrieve relevant chunks from the same document
                # We use document.metadata.filepath as document_id, assuming it's the ID in Chroma
                # Querying with document content to find most salient parts.
                # Note: ChromaRetriever.retrieve_by_filepath expects a query string.
                # We might need a more sophisticated query than just the full content for best results.
                # For now, let's assume document.content is a reasonable query.

                # Creating a generic query string for retrieving chunks from the same document.
                # This could be a summary of the document if available, or just a generic instruction.
                # For now, we'll use a placeholder query for same-document retrieval.
                # A more sophisticated approach might involve embedding specific parts of the document
                # or using keywords.
                same_doc_query = f"Key sections and functionalities within {document.metadata.filepath}"
                same_document_chunks_data = self.chroma_retriever.retrieve_by_filepath(
                    query=same_doc_query,
                    filepath=document.metadata.filepath,
                    n_results=3,  # Max 3 chunks from the same document
                )

                # 2. Retrieve relevant chunks from other documents
                # Using document content as the query to find semantically similar chunks elsewhere.
                other_document_chunks_data = self.chroma_retriever.retrieve_chunks(
                    query=document.content,
                    n_results=8,  # Fetch a bit more to filter out same-doc chunks
                )

                processed_retrieved_chunks = []

                # Process same-document chunks
                for chunk_data in same_document_chunks_data:
                    # Assuming chunk_data is a dict with 'content', 'metadata', etc.
                    # The actual structure depends on what ChromaDBStorage.search_chunks returns
                    # and how ChromaRetriever formats it.
                    # We might need to adjust this based on the actual return type.
                    if chunk_data.get("content"):
                        processed_retrieved_chunks.append(
                            {
                                "source": "same_document",
                                "filepath": chunk_data.get("metadata", {}).get(
                                    "document_id", document.metadata.filepath
                                ),  # or chunk_data.get("document_id")
                                "content_snippet": chunk_data["content"][
                                    :500
                                ],  # Limiting snippet length
                                "relevance_score": chunk_data.get("distance")
                                or chunk_data.get(
                                    "score", 0
                                ),  # Or whatever relevance metric is provided
                            }
                        )

                # Process other-document chunks, ensuring they are not from the same document
                other_docs_count = 0
                for chunk_data in other_document_chunks_data:
                    if other_docs_count >= 5:  # Max 5 chunks from other documents
                        break

                    chunk_filepath = chunk_data.get("metadata", {}).get(
                        "document_id"
                    )  # or chunk_data.get("document_id")
                    if (
                        chunk_filepath
                        and chunk_filepath != document.metadata.filepath
                        and chunk_data.get("content")
                    ):
                        processed_retrieved_chunks.append(
                            {
                                "source": "other_document",
                                "filepath": chunk_filepath,
                                "content_snippet": chunk_data["content"][
                                    :500
                                ],  # Limiting snippet length
                                "relevance_score": chunk_data.get("distance")
                                or chunk_data.get("score", 0),
                            }
                        )
                        other_docs_count += 1

                if processed_retrieved_chunks:
                    # Use cast to resolve the type incompatibility
                    context["retrieved_contextual_chunks"] = cast(
                        Collection[str], processed_retrieved_chunks
                    )

            except Exception as e:
                print(
                    f"Warning: Could not retrieve contextual chunks for {document.metadata.filepath}: {e}"
                )

        return context

    def _process_document_for_generation(
        self, document: Document, project_context: str = ""
    ) -> Tuple[Document, Optional[str], Optional[Dict[str, Any]]]:
        """Processes a single document for documentation generation (intended for parallel execution).

        Args:
            document: Document to generate documentation for
            project_context: Additional context about the project

        Returns:
            A tuple (document, generated_documentation_string, context_dict)
            Returns (document, None, None) if processing is skipped or fails.
        """
        # Check if document should be excluded
        if any(
            pattern in document.metadata.filepath for pattern in self.exclude_patterns
        ):
            # print(f"Skipping {document.metadata.filepath} due to exclude patterns.")
            return document, None, None

        # Check if document has already been processed (idempotency check)
        # This check is more effective in the main loop after results are gathered,
        # especially if re-generating. For now, keeping it here for early exit.
        if document.metadata.filepath in self.processed_files:
            # print(f"Skipping {document.metadata.filepath} as already processed.")
            return document, None, None

        # Build comprehensive context
        # Note: project_context is not directly used by _build_comprehensive_context here,
        # but it was part of the original generate_for_document signature.
        # It might be used if _build_documentation_prompt is further refactored.
        context = self._build_comprehensive_context(document)

        # Build documentation prompt
        prompt = self._build_documentation_prompt(
            document, context
        )  # context here is the rich dictionary

        # Generate documentation using LLM
        try:
            documentation_str = self.llm_client.document_code(
                code=document.content,
                language=str(document.metadata.language).lower(),
                additional_context=prompt,  # prompt is the full string prompt
            )
            if documentation_str:
                return document, documentation_str, context
            else:
                # print(f"LLM returned no documentation for {document.metadata.filepath}.")
                return (
                    document,
                    None,
                    context,
                )  # Return context even if no docs, for potential analysis
        except Exception as e:
            print(
                f"Error generating documentation for {document.metadata.filepath} via LLM: {str(e)}"
            )
            return document, None, None  # Indicate failure

    def generate_for_document(
        self, document: Document, project_context: str = ""
    ) -> Optional[str]:
        """Generate documentation for a document.

        Args:
            document: Document to generate documentation for
            project_context: Additional context about the project

        Returns:
            Generated documentation if successful, None otherwise
        """
        processed_document, documentation_str, context = (
            self._process_document_for_generation(document, project_context)
        )

        if documentation_str and context:
            # Save documentation
            self._save_documentation(processed_document, documentation_str, context)
            self.processed_files.add(processed_document.metadata.filepath)
            # Add document to documents_by_path for later use in search index, etc.
            self.documents_by_path[processed_document.metadata.filepath] = (
                processed_document
            )
            return documentation_str

        # If documentation_str is None but context is not (e.g. LLM returned empty),
        # we might still want to mark as processed to avoid retries on empty files.
        # However, if context is also None, it means it was skipped early or failed hard.
        if (
            context is None
            and processed_document.metadata.filepath not in self.processed_files
        ):
            # This case means it was skipped by exclude patterns or already processed filter *inside* helper
            pass  # Or log if needed
        elif (
            documentation_str is None
            and processed_document.metadata.filepath not in self.processed_files
        ):
            # Potentially an LLM failure or empty generation, but not an early skip.
            # Decide if these should be added to processed_files to prevent retries.
            # For now, only adding to processed_files if _save_documentation was successful.
            pass

        return None

    def _save_documentation(
        self, document: Document, documentation: str, context: Dict[str, Any]
    ) -> None:
        """Save generated documentation.

        Args:
            document: Original document
            documentation: Generated documentation
            context: Comprehensive context used for generation
        """
        # Create output path
        rel_path = os.path.relpath(
            document.metadata.filepath, self.repo_map.root_path if self.repo_map else ""
        )
        output_path = self.docs_dir / f"{rel_path}.md"

        # Create directory if it doesn't exist
        os.makedirs(output_path.parent, exist_ok=True)

        # Build front matter
        front_matter = {
            "title": os.path.basename(document.metadata.filepath),
            "description": f"Documentation for {document.metadata.filepath}",
            "language": document.metadata.language,
        }

        # Add enhanced metadata from context
        if context["module_info"]:
            front_matter.update(context["module_info"])

        # Add code quality metrics
        if context["code_quality"]:
            front_matter["code_quality"] = context["code_quality"]

        # Add documentation statistics
        if context["documentation_stats"]:
            front_matter["documentation_stats"] = context["documentation_stats"]

        # Write documentation with front matter
        with open(output_path, "w") as f:
            f.write("---\n")
            for key, value in front_matter.items():
                f.write(f"{key}: {value}\n")
            f.write("---\n\n")
            f.write(documentation)

        # Update navigation
        self._update_navigation(document, output_path, context)

    def _needs_update(self, document: Document) -> bool:
        """Check if a document needs to be updated.

        Args:
            document: Document to check

        Returns:
            True if the document needs updating, False otherwise
        """
        # Always update if not previously processed
        if document.metadata.filepath not in self.processed_files:
            return True

        # Check if the source file has been modified
        source_path = Path(document.metadata.filepath)
        if not source_path.exists():
            return False

        # Get the corresponding documentation file
        rel_path = self._get_relative_doc_path(document.metadata.filepath)
        doc_path = self._get_output_path(rel_path)

        if not doc_path.exists():
            return True

        # Compare modification times
        source_mtime = source_path.stat().st_mtime
        doc_mtime = doc_path.stat().st_mtime

        return source_mtime > doc_mtime

    def _update_navigation(
        self, document: Document, output_path: Path, context: Dict[str, Any]
    ) -> None:
        """Update the navigation structure for a document.

        Args:
            document: Document to update navigation for
            output_path: Path to the documentation file
            context: Comprehensive context used for generation
        """
        # Create navigation item
        nav_item = {
            "title": os.path.basename(document.metadata.filepath),
            "path": str(output_path.relative_to(self.docs_dir)),
        }

        # Check if item already exists
        for item in self.nav_items:
            if item.get("path") == nav_item["path"]:
                item.update(nav_item)
                return

        # Add new item
        self.nav_items.append(nav_item)

    def _build_documentation_prompt(
        self, document: Document, context: Dict[str, Any]
    ) -> str:
        """Build a detailed documentation prompt for the LLM.

        Args:
            document: Document to document
            context: Additional context about the file as a dictionary

        Returns:
            Prompt for the LLM
        """
        # Convert context dictionary to a formatted string for the prompt
        context_str = self._format_context_dict_for_prompt(context)

        # Base prompt including file information
        prompt = f"""
        # Documentation Request for {document.metadata.filepath}

        ## File Information
        - Language: {document.metadata.language}
        - Lines: {document.metadata.line_count}
        - Symbols: {", ".join(document.metadata.classes + document.metadata.functions)[:100] + ("..." if len(", ".join(document.metadata.classes + document.metadata.functions)) > 100 else "")}
        - Imports: {", ".join(document.metadata.imports)[:100] + ("..." if len(", ".join(document.metadata.imports)) > 100 else "")}

        ## Additional Context
        {context_str}

        ## Documentation Guidelines
        - Generate comprehensive, clear, and accurate documentation for the code.
        - Include explanations of core functionality, architecture, and design patterns.
        - Highlight key classes, functions, and variables with examples where appropriate.
        - Organize documentation logically with headers and sections.
        - Include any relevant warnings, limitations, or performance considerations.
        """

        # Add custom style prompt if provided
        if self.llm_style_prompt:
            prompt += f"\n\n## Style Guidelines\n{self.llm_style_prompt}"

        return prompt

    def _format_context_dict_for_prompt(self, context: Dict[str, Any]) -> str:
        """Format a context dictionary into a string for the LLM prompt.

        Args:
            context: The context dictionary

        Returns:
            Formatted string
        """
        # Convert the complex nested dictionary into a simple textual format
        # that's easier for the LLM to process
        lines = []

        # Module info
        if context.get("module_info"):
            lines.append("### Module Information")
            for key, value in context["module_info"].items():
                lines.append(f"- {key}: {value}")

        # Dependencies
        if context.get("dependencies"):
            lines.append("\n### Dependencies")
            for dep in context["dependencies"][:5]:  # Limit to 5 dependencies
                lines.append(f"- {dep}")
            if len(context["dependencies"]) > 5:
                lines.append(
                    f"- ...and {len(context['dependencies']) - 5} more dependencies"
                )

        # Related files
        if context.get("related_files"):
            lines.append("\n### Related Files")
            for file in context["related_files"][:5]:  # Limit to 5 related files
                lines.append(f"- {file}")
            if len(context["related_files"]) > 5:
                lines.append(
                    f"- ...and {len(context['related_files']) - 5} more related files"
                )

        # Retrieved contextual chunks
        if context.get("retrieved_contextual_chunks"):
            lines.append("\n### Relevant Code Context")
            for i, chunk in enumerate(
                context["retrieved_contextual_chunks"][:3]
            ):  # Limit to 3 chunks
                source = chunk.get("source", "unknown")
                filepath = chunk.get("filepath", "unknown")
                snippet = chunk.get("content_snippet", "")[:200]  # First 200 chars
                lines.append(f"- From {source}: {filepath}")
                lines.append(f"  ```\n  {snippet}...\n  ```")

        return "\n".join(lines)

    def _get_relative_doc_path(self, filepath: str) -> Path:
        """Determine the relative path for documentation file.

        Args:
            filepath: Original file path

        Returns:
            Relative path for documentation
        """
        # Get the file name and directory structure
        basename = os.path.basename(filepath)
        filename, _ = os.path.splitext(basename)

        # Sanitize path components
        sanitized = re.sub(r"[^\w\-\.]", "_", filename)

        # Get directory structure for organizing docs
        dir_path = os.path.dirname(filepath)

        # Create path structure mirroring original with sanitized names
        path_parts = []

        # Split the directory path into components
        components = dir_path.split(os.sep)

        # Filter components to exclude system paths or very common directories
        filtered_components = [
            comp
            for comp in components
            if comp and comp not in [".", "..", "src", "lib", "app", "test", "tests"]
        ]

        # Use last 2 directory levels at most to avoid deep nesting
        if filtered_components:
            path_parts = (
                filtered_components[-2:]
                if len(filtered_components) > 2
                else filtered_components
            )

        # Build the relative path
        if path_parts:
            relative_path = Path(*path_parts) / f"{sanitized}.md"
        else:
            relative_path = Path(f"{sanitized}.md")

        return relative_path

    def _get_output_path(self, rel_path: Path) -> Path:
        """Determine the output path for a documentation file.

        Args:
            rel_path: Relative documentation path

        Returns:
            Full output path
        """
        output_path = self.docs_dir / rel_path

        # Ensure the directory exists
        os.makedirs(output_path.parent, exist_ok=True)

        return output_path

    def generate_for_repository(
        self,
        documents: List[Document],
        repo_name: str = "",
        repo_description: str = "",
        max_workers: Optional[int] = None,
    ) -> None:
        """Generate documentation for an entire repository.

        Args:
            documents: List of documents to document
            repo_name: Name of the repository
            repo_description: Description of the repository
            max_workers: Maximum number of worker threads for parallel processing. Defaults to os.cpu_count().
        """
        total_documents = len(documents)
        project_context = f"Repository: {repo_name}\\n{repo_description}\\nTotal files: {total_documents}\\n"

        if self.documentation_structure == "module_based":
            print("Generating documentation in module-based structure.")
            if not self.repo_map:
                print(
                    "Warning: RepositoryMap is required for module-based documentation. Falling back to file-based."
                )
                self._generate_file_based_documentation(
                    documents, project_context, max_workers
                )
            else:
                self._generate_module_based_documentation(
                    documents, project_context, max_workers
                )
        else:  # Default to file_based
            print("Generating documentation in file-based structure.")
            self._generate_file_based_documentation(
                documents, project_context, max_workers
            )

        # Save configuration (includes all processed files up to this point)
        self._save_configuration()

        # Generate index and overview documents (these are common to both structures for now)
        print("Generating index and overview pages...")
        self._generate_index_page(repo_name, repo_description, documents)
        # self._generate_overview_pages(documents) # This might be redundant or conflict with module pages
        # Consider if _generate_overview_pages needs to be smarter or disabled for module_based structure
        # For now, let's keep it, but it might need review based on how module pages are structured.
        if self.documentation_structure == "file_based":
            self._generate_overview_pages(
                documents
            )  # Only generate old style overviews if file-based

        # Build the documentation structure (MkDocs config, assets etc.)
        print("Building final documentation structure...")
        self.build_documentation()  # This will also need to be aware of the structure for navigation

    def _generate_file_based_documentation(
        self,
        documents: List[Document],
        project_context: str,
        max_workers: Optional[int],
    ) -> None:
        """Helper to encapsulate the original file-based parallel generation logic."""
        total_documents = len(documents)
        if max_workers is None:
            max_workers = os.cpu_count() or 1

        print(
            f"Starting file-based documentation generation for {total_documents} documents using up to {max_workers} workers..."
        )
        results_from_processing = []
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_document = {
                executor.submit(
                    self._process_document_for_generation, doc, project_context
                ): doc
                for doc in documents
            }
            for i, future in enumerate(
                concurrent.futures.as_completed(future_to_document)
            ):
                doc_path = future_to_document[future].metadata.filepath
                try:
                    result_tuple = future.result()
                    results_from_processing.append(result_tuple)
                    print(
                        f"({i + 1}/{total_documents}) Successfully processed file: {doc_path}"
                    )
                except Exception as exc:
                    print(
                        f"({i + 1}/{total_documents}) File {doc_path} generated an exception: {exc}"
                    )

        print(
            f"\nAll files processed. Now saving {len(results_from_processing)} results..."
        )
        successful_generations = 0
        for processed_doc, documentation_str, context_dict in results_from_processing:
            if documentation_str and context_dict:
                try:
                    self._save_documentation(
                        processed_doc, documentation_str, context_dict
                    )
                    self.processed_files.add(processed_doc.metadata.filepath)
                    self.documents_by_path[processed_doc.metadata.filepath] = (
                        processed_doc
                    )
                    successful_generations += 1
                except Exception as e:
                    print(
                        f"Error saving documentation for {processed_doc.metadata.filepath}: {e}"
                    )
        print(
            f"Successfully generated and saved documentation for {successful_generations} files."
        )

    def _generate_module_based_documentation(
        self,
        documents: List[Document],
        project_context: str,
        max_workers: Optional[int],
    ) -> None:
        """Generates documentation with a focus on modules first."""
        print("Module-based documentation generation is starting...")
        # Ensure repo_map is available (already checked by caller, but good practice)
        if not self.repo_map:
            return

        # 1. Identify modules from RepositoryMap
        # This could use repo_map.get_module_overview() or iterate through categorized nodes.
        # For now, let's assume modules are top-level directories or directories identified by repo_map.module_categories

        module_details = (
            self.repo_map.get_module_overview()
        )  # This provides aggregated stats and structure
        # The structure of module_details["modules"] is {category: [file_paths]}
        # Or, we can iterate through the repo_map tree if more direct node access is needed.

        all_module_docs_to_process_individually = []

        if module_details and "modules" in module_details:
            for category, file_paths in module_details["modules"].items():
                if not file_paths:
                    continue  # Skip empty categories

                module_name = category.replace(
                    " ", "_"
                ).lower()  # Sanitize category name for path
                print(f"\nProcessing module: {category} (as {module_name})")

                # Create a list of Document objects belonging to this module
                module_doc_objects = [
                    doc for doc in documents if doc.metadata.filepath in file_paths
                ]
                if not module_doc_objects:
                    print(
                        f"No Document objects found for files in module {category}. Skipping module page."
                    )
                    continue

                # Create a synthetic Document for the module overview page itself
                # The content will be generated by an LLM.
                # The filepath needs to be unique, e.g., modules/<module_name>/index.md
                module_overview_filepath = str(
                    self.docs_dir / "modules" / module_name / "index.md"
                )

                # Avoid collision with actual files if a file is named modules/.../index.md
                # This is a placeholder Document metadata for the overview page.
                module_overview_doc_metadata = DocumentMetadata(
                    filepath=module_overview_filepath,
                    language=DocumentType.MARKDOWN,  # It's a markdown page
                    size_bytes=0,  # Placeholder
                    last_modified=datetime.datetime.now().timestamp(),
                    line_count=0,
                    module_docstring=f"Overview of the {category} module.",
                )
                module_overview_document_content = ""  # To be generated by LLM

                # TODO: Generate actual module overview content using LLM
                # This would involve creating a specific prompt for module summarization
                # For now, creating a placeholder content.
                placeholder_module_summary_prompt = (
                    f"Provide a concise overview of the '{category}' module. "
                    f"It contains files such as: {', '.join(os.path.basename(fp) for fp in file_paths[:5])}. "
                    f"Describe its purpose, key responsibilities, and how it interacts with other parts of the system."
                )
                try:
                    module_overview_document_content = self.llm_client.answer_question(
                        question=f"Generate module overview for '{category}'",
                        context=placeholder_module_summary_prompt,
                    )
                    print(f"Generated overview content for module {category}.")
                except Exception as e:
                    print(f"Error generating LLM summary for module {category}: {e}")
                    module_overview_document_content = f"# {category} Module\n\nOverview generation failed. Files: {', '.join(file_paths)}"

                module_overview_doc = Document(
                    content=module_overview_document_content,
                    metadata=module_overview_doc_metadata,
                    embedding_id=str(uuid.uuid4()),
                )

                # Save this module overview page
                # The _save_documentation method expects a context dict, we can create a minimal one
                minimal_context = self._build_comprehensive_context(
                    module_overview_doc
                )  # Build context for this synthetic doc
                minimal_context["module_info"] = {
                    "category": category,
                    "is_module_overview": True,
                }
                self._save_documentation(
                    module_overview_doc,
                    module_overview_document_content,
                    minimal_context,
                )
                self.processed_files.add(module_overview_filepath)  # Mark as processed
                self.documents_by_path[module_overview_filepath] = (
                    module_overview_doc  # Add to documents map
                )

                # Collect individual files within this module for later processing if detailed docs are needed
                # For now, let's assume we always process them.
                all_module_docs_to_process_individually.extend(module_doc_objects)
        else:
            print(
                "No module structure found in RepositoryMap or get_module_overview(). Defaulting to processing all files individually."
            )
            all_module_docs_to_process_individually.extend(documents)

        # 2. Process individual files (either all, or those within modules, depending on strategy)
        if all_module_docs_to_process_individually and self.module_doc_depth == "full":
            print(
                f"\nProceeding to generate full documentation for {len(all_module_docs_to_process_individually)} individual files within modules..."
            )
            # Reuse the file-based generation logic for these collected documents
            self._generate_file_based_documentation(
                all_module_docs_to_process_individually, project_context, max_workers
            )
        elif (
            all_module_docs_to_process_individually
            and self.module_doc_depth == "overview_only"
        ):
            print(
                "Module documentation depth set to 'overview_only'. Skipping individual file generation within modules."
            )
        else:
            print("No individual files to process or module_doc_depth prevents it.")

    def _generate_index_page(
        self, repo_name: str, repo_description: str, documents: List[Document]
    ) -> None:
        """Generate the index page for the documentation.

        Args:
            repo_name: Name of the repository
            repo_description: Description of the repository
            documents: List of documents
        """
        # Prepare context for index generation
        languages = set(str(doc.metadata.language) for doc in documents)
        file_types = [os.path.splitext(doc.metadata.filepath)[1] for doc in documents]
        file_type_counts: Dict[str, int] = {}

        for ext in file_types:
            if ext:
                file_type_counts[ext] = file_type_counts.get(ext, 0) + 1

        # Build index prompt
        index_prompt = f"""
Generate a comprehensive markdown index page for a software project documentation site.

Repository: {repo_name}
Description: {repo_description}
Total files: {len(documents)}
Programming languages: {", ".join(languages)}
File distribution: {str(file_type_counts)}

This index page should:
1. Start with a clear introduction to the project
2. Include an overview of the project's purpose and features
3. Provide a high-level explanation of the project architecture
4. Include a 'Getting Started' section with basic usage instructions
5. Explain how the documentation is organized
6. Use proper markdown formatting for MkDocs with Material theme

The documentation site will serve as the main reference for developers working with this codebase.
"""

        # Generate index content
        index_content = self.llm_client.answer_question(
            question="Generate a project documentation index page", context=index_prompt
        )

        # Save the index page
        index_path = self.docs_dir / "index.md"

        # Add front matter
        front_matter = {
            "title": repo_name or "Project Documentation",
            "summary": repo_description or "Project documentation generated by Docstra",
        }

        formatted_content = f"""---
{yaml.dump(front_matter, default_flow_style=False)}
---

{index_content}
"""

        with open(index_path, "w") as f:
            f.write(formatted_content)

        # Add to navigation
        self.nav_items.insert(0, {"title": "Home", "path": "index.md"})

    def _generate_overview_pages(self, documents: List[Document]) -> None:
        """Generate overview pages for the documentation.

        Args:
            documents: List of documents
        """
        # Group documents by directories/modules
        document_groups = self._group_documents_by_directory(documents)

        # Generate overview for each group
        for dir_path, docs in document_groups.items():
            # Skip if fewer than 2 documents
            if len(docs) < 2:
                continue

            # Create a sanitized directory name
            dir_name = os.path.basename(dir_path)
            sanitized_name = re.sub(r"[^\w\-\.]", "_", dir_name)

            # Prepare overview directory
            overview_dir = self.docs_dir / "overview"
            os.makedirs(overview_dir, exist_ok=True)

            # Prepare context for overview generation
            doc_names = [os.path.basename(doc.metadata.filepath) for doc in docs]

            overview_prompt = f"""
Generate a module overview documentation page in markdown format for the '{dir_name}' module/directory.

Files in this module:
{", ".join(doc_names)}

This overview should:
1. Explain the purpose and responsibility of this module/directory
2. Describe how the files work together
3. Highlight key components, classes, or functions
4. Explain the module's role in the overall architecture
5. Include any relevant usage patterns or examples
6. Use proper markdown formatting for MkDocs with Material theme

The overview should help developers understand the organization and purpose of this code module.
"""

            # Generate overview content
            overview_content = self.llm_client.answer_question(
                question=f"Generate a module overview for '{dir_name}'",
                context=overview_prompt,
            )

            # Save the overview page
            overview_path = overview_dir / f"{sanitized_name}.md"
            # Add front matter
            front_matter = {
                "title": f"{dir_name} Module",
                "summary": f"Overview of the {dir_name} module and its components",
            }

            formatted_content = f"""---
{yaml.dump(front_matter, default_flow_style=False)}
---

{overview_content}
"""

            with open(overview_path, "w") as f:
                f.write(formatted_content)

            # Add to navigation
            self.nav_items.append(
                {
                    "title": f"{dir_name} Overview",
                    "path": f"overview/{sanitized_name}.md",
                }
            )

    def _group_documents_by_directory(
        self, documents: List[Document]
    ) -> Dict[str, List[Document]]:
        """Group documents by their directory paths.

        Args:
            documents: List of documents

        Returns:
            Dictionary mapping directory paths to lists of documents
        """
        groups: Dict[str, List[Document]] = {}

        for doc in documents:
            dir_path = os.path.dirname(doc.metadata.filepath)
            if dir_path not in groups:
                groups[dir_path] = []

            groups[dir_path].append(doc)

        return groups

    def build_documentation(self) -> None:
        """Build the final documentation structure."""
        # Generate MkDocs configuration
        self._generate_mkdocs_config()

        # Generate search index for better search functionality
        self._generate_search_index()

        # Create custom assets (CSS, JS, etc.)
        self._create_custom_assets()

        # Build MkDocs site if using MkDocs format
        if self.format == "mkdocs":
            self._build_mkdocs_site()

    def _generate_mkdocs_config(self) -> None:
        """Generate MkDocs configuration file."""
        # Organize navigation structure
        nav = self._organize_navigation()

        # Create MkDocs configuration
        mkdocs_config = {
            "site_name": "Project Documentation",
            "theme": {
                "name": "material",
                "palette": {"primary": "indigo", "accent": "indigo"},
                "features": [
                    "navigation.instant",
                    "navigation.tracking",
                    "navigation.expand",
                    "navigation.indexes",
                    "search.highlight",
                    "search.share",
                    "toc.follow",
                    "content.code.copy",
                ],
                "icon": {"repo": "fontawesome/brands/github"},
            },
            "markdown_extensions": [
                "pymdownx.highlight",
                "pymdownx.superfences",
                "pymdownx.inlinehilite",
                "pymdownx.tabbed",
                "pymdownx.critic",
                "pymdownx.tasklist",
                "admonition",
                "toc",
                "tables",
            ],
            "plugins": ["search", "mkdocstrings"],
            "extra_css": ["assets/css/custom.css"],
            "extra_javascript": ["assets/js/custom.js"],
            "nav": nav,
        }

        # Write MkDocs configuration to file
        config_path = self.output_dir / "mkdocs.yml"
        with open(config_path, "w") as f:
            yaml.dump(mkdocs_config, f, default_flow_style=False)

    def _organize_navigation(self) -> List:
        """Organize navigation items into a structured hierarchy.

        Returns:
            List of navigation items for MkDocs
        """
        nav = []

        # 1. Home page (always first)
        home_path = "index.md"
        if (self.docs_dir / home_path).exists():
            nav.append({"Home": home_path})
        else:  # Try to find any index.md as home
            found_home = list(self.docs_dir.glob("index.md"))  # Top level index
            if not found_home:
                found_home = list(self.docs_dir.glob("**/index.md"))  # Any index.md
            if found_home:
                nav.append({"Home": str(found_home[0].relative_to(self.docs_dir))})
            # If no index.md, home will be implicitly the first item later

        # 2. Standard static sections (customize as needed)
        static_sections = {
            "Getting Started": [
                "getting-started/installation.md",
                "getting-started/quickstart.md",
                "getting-started/configuration.md",
            ],
            "Advanced Topics": [
                "advanced/performance.md",
                "advanced/security.md",
                "advanced/deployment.md",
                "advanced/troubleshooting.md",
            ],
            "Contributing": [
                "contributing/development.md",
                "contributing/code-style.md",
                "contributing/testing.md",
                "contributing/documentation.md",
            ],
        }
        for section_title, section_files in static_sections.items():
            section_nav: Dict[str, List[Dict[str, str]]] = {section_title: []}
            has_content = False
            for file_path_str in section_files:
                if (self.docs_dir / file_path_str).exists():
                    # Title from filename, or define explicitly if needed
                    item_title = Path(file_path_str).stem.replace("_", " ").title()
                    section_nav[section_title].append({item_title: file_path_str})
                    has_content = True
            if has_content:
                # Use cast to ensure type compatibility with nav list
                nav.append(cast(Dict[str, str], section_nav))

        # 3. Documentation content (Modules or Files)
        if self.documentation_structure == "module_based":
            modules_dir = self.docs_dir / "modules"
            if modules_dir.exists() and modules_dir.is_dir():
                module_nav_items = []
                # Iterate over module directories (e.g., modules/core, modules/utils)
                for module_dir in sorted(modules_dir.iterdir()):
                    if module_dir.is_dir():
                        module_name_display = module_dir.name.replace("_", " ").title()
                        module_index_file = module_dir / "index.md"
                        module_files_nav = []
                        if module_index_file.exists():
                            # Add the module overview page first
                            module_files_nav.append(
                                {
                                    f"{module_name_display} Overview": str(
                                        module_index_file.relative_to(self.docs_dir)
                                    )
                                }
                            )

                        # Add other files within the module directory, if module_doc_depth is 'full'
                        if self.module_doc_depth == "full":
                            for item_path in sorted(module_dir.glob("**/*.md")):
                                if (
                                    item_path.name == "index.md"
                                ):  # Already added or will be as overview
                                    continue
                                item_rel_path = str(
                                    item_path.relative_to(self.docs_dir)
                                )
                                item_title = item_path.stem.replace("_", " ").title()
                                if (
                                    item_path.parent != module_dir
                                ):  # It's in a subdirectory
                                    # Handle one level of subdirectories for simplicity
                                    sub_dir_name = item_path.parent.name.replace(
                                        "_", " "
                                    ).title()
                                    # Check if this sub_dir_name section already exists
                                    found_sub_section = False
                                    for sub_item in module_files_nav:
                                        if (
                                            isinstance(sub_item, dict)
                                            and sub_dir_name in sub_item
                                            and isinstance(sub_item[sub_dir_name], list)
                                        ):
                                            # Use a variable with the right type
                                            item_list = sub_item[sub_dir_name]
                                            if isinstance(item_list, list):
                                                item_list.append(
                                                    {item_title: item_rel_path}
                                                )
                                            found_sub_section = True
                                            break
                                    if not found_sub_section:
                                        # nav_entry is a dict of lists, flatten to Dict[str, str] entries
                                        for d in [{item_title: item_rel_path}]:
                                            module_files_nav.append(d)
                                else:
                                    module_files_nav.append({item_title: item_rel_path})

                        if (
                            module_files_nav
                        ):  # If only overview or also other files were added
                            if len(module_files_nav) == 1 and list(
                                module_files_nav[0].values()
                            )[0].endswith("index.md"):
                                # Only an overview page, lift it up without a sub-section for the module itself
                                module_nav_items.append(module_files_nav[0])
                            else:
                                # Create a sub-module dictionary with string: list mapping
                                module_nav_dict = {}
                                module_nav_dict[module_name_display] = module_files_nav
                                module_nav_items.append(
                                    cast(Dict[str, str], module_nav_dict)
                                )

                    if module_nav_items:
                        # Create a modules dictionary with string: list mapping
                        modules_nav_dict = {}
                        modules_nav_dict["Modules"] = module_nav_items
                        nav.append(cast(Dict[str, str], modules_nav_dict))

        # If not module_based, or if there are other files not covered by modules:
        # Use _group_navigation_by_directory for remaining files.
        # This part needs careful consideration for module_based to avoid duplication.
        # For now, assume module_based will primarily use the "Modules" section.
        if self.documentation_structure == "file_based":
            # Add overview pages if any exist (old style)
            overview_dir = self.docs_dir / "overview"
            if overview_dir.exists() and any(overview_dir.iterdir()):
                overview_nav_items = []
                for overview_file in sorted(overview_dir.glob("*.md")):
                    rel_path = str(overview_file.relative_to(self.docs_dir))
                    title = overview_file.stem.replace("_", " ").title()
                    overview_nav_items.append({title: rel_path})
                if overview_nav_items:
                    # Create an overviews dictionary with string: list mapping
                    overviews_nav_dict = {}
                    overviews_nav_dict["Overviews"] = overview_nav_items
                    nav.append(cast(Dict[str, str], overviews_nav_dict))

            # Group remaining documentation files by directory and type
            # Filter self.nav_items to only those not part of special sections already potentially added
            # self.nav_items is populated by _save_documentation. We need a reliable way to get all generated .md paths
            processed_doc_paths = [
                item["path"] for item in self.nav_items if "path" in item
            ]

            # Get all .md files in docs_dir, excluding special dirs handled above
            all_md_files = []
            for md_file in self.docs_dir.glob("**/*.md"):
                rel_path_str = str(md_file.relative_to(self.docs_dir))
                # Exclude files already handled by Home, static sections, overviews (if file_based)
                if rel_path_str == home_path:
                    continue
                is_static = False
                for files in static_sections.values():
                    if rel_path_str in files:
                        is_static = True
                        break
                if is_static:
                    continue
                if (
                    rel_path_str.startswith("overview/")
                    and self.documentation_structure == "file_based"
                ):
                    continue

                all_md_files.append(md_file)

            grouped_nav = self._group_files_for_navigation(all_md_files)
            nav.extend(
                grouped_nav
            )  # Use extend as _group_files_for_navigation returns a list of groups

        # (Optional) Add API Reference section if mkdocstrings is used and it auto-populates
        # Or if specific files are tagged for API

        return nav

    def _group_files_for_navigation(
        self, file_paths: List[Path]
    ) -> List[Dict[str, str]]:
        """Groups a list of file Paths into a navigation structure for MkDocs.
           Replaces _group_navigation_by_directory to work with actual file paths.
        Args:
            file_paths: List of Path objects for markdown files relative to docs_dir.
        Returns:
            List of grouped navigation items.
        """
        grouped: Dict[str, List[Dict[str, str]]] = {}
        for file_path in file_paths:
            rel_path = str(file_path.relative_to(self.docs_dir))
            title = file_path.stem.replace("_", " ").title()
            parts = file_path.relative_to(self.docs_dir).parts

            if len(parts) > 1:  # Has directory structure (parts includes filename)
                group_name = parts[0].replace("_", " ").title()
                if group_name not in grouped:
                    grouped[group_name] = []
                grouped[group_name].append({title: rel_path})
            else:  # Top-level file
                if "General" not in grouped:
                    grouped["General"] = []
                grouped["General"].append({title: rel_path})

        result: List[Dict[str, str]] = []
        for group_name, items in sorted(grouped.items()):
            for item in items:
                result.append(item)
        return result

    def _generate_search_index(self) -> None:
        """Generate enhanced search index for documentation."""
        # MkDocs will generate its own search index, but we can enhance it
        # with additional metadata and content for better search functionality

        search_data = []

        # Process each document to extract searchable content
        for filepath, document in self.documents_by_path.items():
            # Get corresponding documentation file
            rel_path = self._get_relative_doc_path(filepath)
            doc_path = str(rel_path)

            # Extract searchable metadata
            item = {
                "location": doc_path,
                "title": os.path.basename(filepath),
                "text": "",
                "keywords": [],
            }

            # Extract symbols for keywords
            item["keywords"] = []  # Initialize as empty list

            # Ensure item["keywords"] is a proper list
            if "keywords" not in item:
                item["keywords"] = []
            elif not isinstance(item["keywords"], list):
                # Cast any sequence type to a list
                if hasattr(item["keywords"], "__iter__"):
                    item["keywords"] = list(item["keywords"])
                else:
                    item["keywords"] = []

            # Define a helper function to avoid redundant code
            def add_symbols_to_keywords(symbols_data: object) -> None:
                """Helper function to add symbols to keywords list safely."""
                if symbols_data:
                    if isinstance(item["keywords"], list):
                        if isinstance(symbols_data, dict):
                            for key in symbols_data.keys():
                                if key not in item["keywords"]:
                                    item["keywords"].append(key)
                        elif hasattr(symbols_data, "__iter__") and not isinstance(
                            symbols_data, str
                        ):
                            for symbol in symbols_data:
                                if symbol not in item["keywords"]:
                                    item["keywords"].append(symbol)

            # Process symbols dictionary
            if hasattr(document.metadata, "symbols"):
                add_symbols_to_keywords(getattr(document.metadata, "symbols"))

            # Process classes list
            if hasattr(document.metadata, "classes"):
                add_symbols_to_keywords(getattr(document.metadata, "classes"))

            # Process functions list
            if hasattr(document.metadata, "functions"):
                add_symbols_to_keywords(getattr(document.metadata, "functions"))

            # Extract summary text from the first few lines of content
            content_lines = document.content.split("\n")
            item["text"] = "\n".join(content_lines[:20])

            search_data.append(item)

        # Save the enhanced search data
        search_path = self.docs_dir / "assets" / "js" / "extra-search-data.json"
        with open(search_path, "w") as f:
            json.dump(search_data, f)

    def _create_custom_assets(self) -> None:
        """Create custom assets for the documentation site."""
        # Create custom CSS
        custom_css = """
/* Custom styles to enhance MkDocs Material theme */

/* Improve code block styling */
.md-typeset pre > code {
    border-radius: 4px;
}

/* Add styling for class and function cards */
.docstra-class,
.docstra-function {
    padding: 1em;
    margin-bottom: 1.5em;
    border-left: 4px solid var(--md-primary-fg-color);
    background-color: rgba(0, 0, 0, 0.025);
}

.docstra-class h3,
.docstra-function h3 {
    margin-top: 0;
    color: var(--md-primary-fg-color);
}

/* Source file highlight */
.docstra-source {
    margin-top: 2em;
    padding-top: 1em;
    border-top: 1px solid rgba(0, 0, 0, 0.1);
}

/* Parameter tables */
.docstra-params {
    font-size: 0.9em;
}

.docstra-params th {
    background-color: rgba(0, 0, 0, 0.05);
}

/* Type annotations */
.docstra-type {
    color: var(--md-code-fg-color);
    font-family: var(--md-code-font-family);
    font-size: 0.9em;
}

/* Enhance admonitions */
.md-typeset .admonition {
    font-size: 0.9em;
}
"""

        # Write custom CSS
        custom_css_path = self.css_dir / "custom.css"
        with open(custom_css_path, "w") as f:
            f.write(custom_css)

        # Create custom JavaScript
        custom_js = """
document.addEventListener('DOMContentLoaded', function() {
    // Enable the enhanced search if available
    const extraSearchData = document.querySelector('script[src$="extra-search-data.json"]');
    if (extraSearchData) {
        // Load and process the enhanced search data
        fetch(extraSearchData.getAttribute('src'))
            .then(response => response.json())
            .then(data => {
                window.docstraExtraSearchData = data;
                console.log('Enhanced search data loaded');
            })
            .catch(err => console.error('Error loading enhanced search data:', err));
    }
    
    // Add syntax highlighting enhancements
    document.querySelectorAll('pre code').forEach(block => {
        // Add line numbers if not already present
        if (!block.classList.contains('linenos')) {
            const lineNumbers = block.innerHTML.split('\\n').length;
            if (lineNumbers > 3) {
                block.classList.add('line-numbers');
            }
        }
    });
});
"""

        # Write custom JavaScript
        custom_js_path = self.js_dir / "custom.js"
        with open(custom_js_path, "w") as f:
            f.write(custom_js)

    def _build_mkdocs_site(self) -> None:
        """Build the MkDocs site."""
        try:
            # Check if MkDocs is installed
            subprocess.run(["mkdocs", "--version"], check=True, capture_output=True)

            # Build the site
            subprocess.run(["mkdocs", "build"], cwd=self.output_dir, check=True)

            print(f"MkDocs site built successfully in {self.output_dir}/site/")
        except subprocess.CalledProcessError:
            print("Error: MkDocs is not installed or not available in PATH.")
            print(
                "Install MkDocs using: pip install mkdocs mkdocs-material pymdown-extensions mkdocstrings"
            )
        except Exception as e:
            print(f"Error building MkDocs site: {str(e)}")

    def serve_documentation(self, port: int = 8000) -> None:
        """Serve the documentation using MkDocs.

        Args:
            port: Port to serve the documentation on
        """
        try:
            # Check if MkDocs is installed
            subprocess.run(["mkdocs", "--version"], check=True, capture_output=True)

            # Serve the site
            print(f"Starting documentation server at: http://localhost:{port}")
            subprocess.run(
                ["mkdocs", "serve", "-a", f"localhost:{port}"], cwd=self.output_dir
            )
        except subprocess.CalledProcessError:
            print("Error: MkDocs is not installed or not available in PATH.")
            print(
                "Install MkDocs using: pip install mkdocs mkdocs-material pymdown-extensions mkdocstrings"
            )

            # Fall back to Python's simple HTTP server
            print("Falling back to Python's HTTP server...")
            site_dir = self.output_dir / "site"

            if not site_dir.exists():
                # Try to build the site first
                try:
                    subprocess.run(["mkdocs", "build"], cwd=self.output_dir, check=True)
                except Exception:
                    print("Error building site. Serving the docs directory instead.")
                    site_dir = self.docs_dir

            # Serve using Python's HTTP server
            current_dir = os.getcwd()
            os.chdir(site_dir)

            try:
                import http.server
                import socketserver

                handler = http.server.SimpleHTTPRequestHandler
                socketserver.TCPServer.allow_reuse_address = True

                with socketserver.TCPServer(("", port), handler) as httpd:
                    print(f"Serving at http://localhost:{port}")
                    httpd.serve_forever()
            finally:
                os.chdir(current_dir)
        except Exception as e:
            print(f"Error serving documentation: {str(e)}")
